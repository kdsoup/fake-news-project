{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_to_lower(match_obj):\n",
    "    \"\"\"\n",
    "    Replacement function to convert uppercase letter to lowercase.\n",
    "    Code reference: https://pynative.com/python-regex-replace-re-sub/\n",
    "    \"\"\"\n",
    "    if match_obj.group() is not None:\n",
    "        return match_obj.group().lower()\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Takes a raw input data string, and returns a modified version, so that all words are lower case, \n",
    "    multiple white spaces, tabs, and new lines are removed. Numbers, date, emails, and URLs are \n",
    "    replaced by <NUM>, <DATE>, <EMAIL>, and <URL>.\n",
    "    \"\"\"\n",
    "\n",
    "    # To lowercase\n",
    "    caps = re.compile(r'[A-Z]')\n",
    "    text = caps.sub(convert_to_lower, text)\n",
    "\n",
    "    # Remove multiple white space, taps, and new lines\n",
    "    newline = re.compile(r'\\n')\n",
    "    spacing = re.compile(r' {2,}')\n",
    "    text = newline.sub(r' ', text) \n",
    "    text = spacing.sub(r' ', text)\n",
    "\n",
    "    # Replace numbers, dates, email, and URLs with <NUM>, <DATE>, <EMAIL>, <URL>\n",
    "    # Email\n",
    "    p_email = re.compile(r'[a-zA-Z]*@[a-zA-Z]*\\.[a-zA-Z]*\\.?[a-zA-Z]*')\n",
    "    text = p_email.sub(\"<EMAIL>\", text)\n",
    "\n",
    "    # URL\n",
    "    p_url = re.compile(r'(https?[a-z/:\\.\\-0-9_]*)')\n",
    "    text = p_url.sub(\"<URL>\", text)\n",
    "\n",
    "    # Dates\n",
    "    p_date = re.compile(r'([0-9]{4}-?[0-9]{2}-[0-9]{2} ?[0-9:\\.]*)')\n",
    "    text = p_date.sub(\"<DATE>\", text)\n",
    "\n",
    "    # Numbers (incl. floats)\n",
    "    p_num = re.compile(r'[0-9]+\\.?[0-9]*')\n",
    "    text = p_num.sub(\"<NUM>\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load news_sample.csv file from git source\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv')\n",
    "\n",
    "# cleanup text on 'content' column and add into new column 'content_clean'\n",
    "for i in range(len(df.index)):\n",
    "    df.at[i, 'content_clean'] = clean_text(df.content[i])\n",
    "\n",
    "# save cleaned up data to csv file\n",
    "df.to_csv(\"data/news_sample_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kristian/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kristian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# tokenize the text\n",
    "# THIS IS NOT IN USE FOR NOW\n",
    "def tokenize_text(text: str) -> list:\n",
    "    return word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(filename: str) -> None:\n",
    "    df = pd.read_csv(filename)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # add stopword filtered text to a new column\n",
    "    for i in range(len(df.index)):\n",
    "        filtered_sentence = []\n",
    "        word_tokens = word_tokenize(df.content_clean[i])\n",
    "        for w in word_tokens:\n",
    "            if w not in stop_words:\n",
    "                filtered_sentence.append(w)\n",
    "        \n",
    "        nostop_text = ' '.join(filtered_sentence)\n",
    "        df.at[i, 'content_stopword'] = nostop_text\n",
    "\n",
    "    # write dataframe to new csv file\n",
    "    df.to_csv(filename)\n",
    "    # df.to_csv(filename[0:-4] + \"_sw.csv\")\n",
    "\n",
    "remove_stopwords(\"data/news_sample_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute stopword reduction rate. add vocabulary columns and reduction rate column to csv file\n",
    "def stopword_reduction_rate(filename: str) -> None:\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    for i in range(len(df.index)):\n",
    "        content_clean_vocabulary_size = len(set(word_tokenize(df.content_clean[i])))\n",
    "        content_stopword_vocabulary_size = len(set(word_tokenize(df.content_stopword[i])))\n",
    "        decrease = content_clean_vocabulary_size - content_stopword_vocabulary_size\n",
    "        reduction_rate = (decrease/content_clean_vocabulary_size) * 100\n",
    "        df.at[i, 'content_clean_vocabulary_size'] = content_clean_vocabulary_size\n",
    "        df.at[i, 'content_stopword_vocabulary_size'] = content_stopword_vocabulary_size\n",
    "        df.at[i, 'content_stopword_reduction_rate'] = round(reduction_rate, 3)\n",
    "    \n",
    "    df.to_csv(filename)\n",
    "    \n",
    "stopword_reduction_rate('data/news_sample_cleaned.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# remove word variations\n",
    "def remove_word_variations(filename: str) -> None:\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    for i in range(len(df.index)):\n",
    "        stemmed_words = []\n",
    "        word_tokens = word_tokenize(df.content_stopword[i])\n",
    "        for w in word_tokens:\n",
    "            stemmed_words.append(stemmer.stem(w))\n",
    "        \n",
    "        stemmed_text = ' '.join(stemmed_words)\n",
    "        df.at[i, 'content_stem'] = stemmed_text\n",
    "    \n",
    "    df.to_csv(filename)\n",
    "\n",
    "remove_word_variations('data/news_sample_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute size of vocabulary after stemming. Add column with 'stem vocabulary size' to the csv file\n",
    "def stemming_reduction_rate(filename: str) -> None:\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    for i in range(len(df.index)):\n",
    "        stem_vocabulary_size = len(set(word_tokenize(df.content_stem[i])))\n",
    "        clean_vocabulary_size = len(set(word_tokenize(df.content_clean[i])))\n",
    "        decrease = clean_vocabulary_size - stem_vocabulary_size\n",
    "        reduction_rate = (decrease/clean_vocabulary_size) * 100\n",
    "        df.at[i, 'content_stem_vocabulary_size'] = stem_vocabulary_size\n",
    "        df.at[i, 'content_stem_reduction_rate'] = round(reduction_rate, 3)\n",
    "\n",
    "    df.to_csv(filename)\n",
    "\n",
    "stemming_reduction_rate('data/news_sample_cleaned.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
