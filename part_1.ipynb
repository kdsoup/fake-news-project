{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Preprocessing 'news_sample.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lib.process_c as process_c\n",
    "\n",
    "# load 'news_sample.csv' file from git source\n",
    "df_sample = pd.read_csv('https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv')\n",
    "\n",
    "# Apply preprocess to dataframe: cleanup -> remove stopword -> stemming\n",
    "# process_a.preprocess(df_sample)\n",
    "process_c.preprocess(df_sample)\n",
    "\n",
    "# save csv file copy of preprocessed dataframe\n",
    "df_sample.to_csv(\"data/news_sample_cleaned.csv\")\n",
    "\n",
    "# pd.reset_option('display.max_rows')\n",
    "# pd.set_option('display.max_colwidth', 150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary sizes and reduction rates\n",
    "\n",
    "The vocabulary sizes and reduction rates are computed in the preprocess method c. The results are stored directly to the input dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make at least three non-trivial observations/discoveries about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df_new = pd.read_csv('data/995,000_rows.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Basic Statistics of the Dataset:\")\n",
    "# print(df.info())\n",
    "\n",
    "# # Missing Values\n",
    "# print(\"\\nMissing Values in Each Column:\")\n",
    "# print(df.isnull().sum())\n",
    "\n",
    "# Handle NaN values in the 'content' column\n",
    "df_new['content'] = df_new['content'].fillna('')\n",
    "      \n",
    "# Apply clean_text function\n",
    "df_new['content_clean'] = df_new['content'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count URLs with <URL> tag\n",
    "def count_urls_with_tag(text):\n",
    "    urls_with_tag = re.findall('<url>', text)\n",
    "    return len(urls_with_tag)\n",
    "\n",
    "# Apply count URLs with <URL> tag\n",
    "df_new['url_count'] = df_new['content_clean'].apply(count_urls_with_tag)\n",
    "\n",
    "# print(df[['content_clean', 'url_count_with_tag']])\n",
    "\n",
    "# Total URLs in Content\n",
    "total_urls = df_new['url_count'].sum()\n",
    "print(\"Total URLs in Content:\", total_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count DATEs with <DATE> tag\n",
    "def count_date_with_tag(text):\n",
    "    date_with_tag = re.findall('<date>', text)\n",
    "    return len(date_with_tag)\n",
    "\n",
    "# Apply count DATEs with <DATE> tag\n",
    "df_new['date_count'] = df_new['content_clean'].apply(count_date_with_tag)\n",
    "\n",
    "# Total DATEs in Content\n",
    "total_dates = df_new['date_count'].sum()\n",
    "print(\"Total DATEs in Content:\", total_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to count NUMs with <NUM> tag\n",
    "def count_num_with_tag(text):\n",
    "    num_with_tag = re.findall('<num>', text)\n",
    "    return len(num_with_tag)\n",
    "\n",
    "# Apply count NUms with <NUM> tag\n",
    "df_new['num_count'] = df_new['content_clean'].apply(count_num_with_tag)\n",
    "\n",
    "# Total NUMs in Content\n",
    "total_nums = df_new['num_count'].sum()\n",
    "print(\"Total NUMs in Content:\", total_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize an empty counter to store word frequencies\n",
    "clean_word_freq = Counter()\n",
    "\n",
    "# Iterate over each row of the DataFrame\n",
    "for _, row in df_new.iterrows():\n",
    "    # Join the clean words in the 'content_clean' column of the current row into a single string\n",
    "    clean_text = ' '.join(re.findall(r'\\b\\w+\\b', row['content_clean']))\n",
    "    \n",
    "    # Count the word frequencies for the current row\n",
    "    clean_word_freq.update(clean_text.split())\n",
    "\n",
    "# Sort the word frequencies in descending order\n",
    "sorted_clean_word_freq = sorted(clean_word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Extract the 100 most frequent words\n",
    "top_100_clean_words = sorted_clean_word_freq[:100]\n",
    "\n",
    "# Print the top 100 most frequent words\n",
    "print(top_100_clean_words)\n",
    "\n",
    "# Extract the 10000 most frequent words\n",
    "top_10000_clean_words = sorted_clean_word_freq[:10000]\n",
    "\n",
    "# Barplot for top 10000 most frequent clean words\n",
    "plt.figure(figsize=(15, 6))\n",
    "words, frequencies = zip(*top_10000_clean_words)\n",
    "plt.bar(words, frequencies)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10000 Most Frequent Clean Words')\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply remove_stopwords to 'content_clean' column and create 'content_stopword' column\n",
    "df_new['content_stopword'] = df_new['content_clean'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['content_stem'] = df_new['content_stopword'].apply(remove_word_variations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize an empty counter to store word frequencies\n",
    "clean_word_freq_after = Counter()\n",
    "\n",
    "# Iterate over each row of the DataFrame\n",
    "for _, row in df_new.iterrows():\n",
    "    # Join the clean words in the 'content_clean' column of the current row into a single string\n",
    "    clean_text_after = ' '.join(re.findall(r'\\b\\w+\\b', row['content_stem']))\n",
    "    \n",
    "    # Count the word frequencies for the current row\n",
    "    clean_word_freq_after.update(clean_text.split())\n",
    "\n",
    "# Sort in descending order\n",
    "sorted_clean_word_freq_after = sorted(clean_word_freq_after.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Extract the 100 most frequent words\n",
    "top_100_clean_words_after = sorted_clean_word_freq_after[:100]\n",
    "\n",
    "for word, frequency in top_100_clean_words_after:\n",
    "    print(f\"{word}: {frequency}\")\n",
    "\n",
    "# Extract the 10000 most frequent words\n",
    "top_10000_clean_words_after = sorted_clean_word_freq_after[:10000]\n",
    "\n",
    "# Barplot for top 10000 most frequent clean words\n",
    "plt.figure(figsize=(12, 6))\n",
    "words, frequencies = zip(*top_10000_clean_words_after)\n",
    "plt.bar(words, frequencies)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10000 Most Frequent Clean Words After Preprocessing')\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Apply preprocess to '995,000_rows.csv' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lib.process_b as process_b\n",
    "\n",
    "src = 'data/995,000_rows.csv'\n",
    "# src = 'data/news_sample.csv'\n",
    "dst = src[0:-4] + '_cleaned.csv'\n",
    "\n",
    "# preprocess\n",
    "df = pd.read_csv(src)\n",
    "process_b.preprocess(df)\n",
    "\n",
    "# save csv file copy of preprocessed dataframe\n",
    "df.to_csv(dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Split the dataset into a training, validation, and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lib.split as split\n",
    "\n",
    "src = 'data/995,000_rows.csv'\n",
    "# src = 'data/995,000_rows_SAMPLE.csv'\n",
    "# src = 'data/news_sample.csv'\n",
    "\n",
    "df = pd.read_csv(src)\n",
    "split.eighty_ten_ten(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
