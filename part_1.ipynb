{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Preprocessing 'news_sample.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lib.process_a as process_a\n",
    "import lib.process_c as process_c\n",
    "\n",
    "# load 'news_sample.csv' file from git source\n",
    "df_sample = pd.read_csv('https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv')\n",
    "\n",
    "# Apply preprocess to dataframe: cleanup -> remove stopword -> stemming\n",
    "# process_a.preprocess(df_sample)\n",
    "process_c.preprocess(df_sample)\n",
    "\n",
    "# save csv file copy of preprocessed dataframe\n",
    "df_sample.to_csv(\"data/news_sample_cleaned.csv\")\n",
    "\n",
    "# pd.reset_option('display.max_rows')\n",
    "# pd.set_option('display.max_colwidth', 150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary sizes and reduction rates\n",
    "\n",
    "The vocabulary sizes and reduction rates are computed in the preprocess method c. The results are stored directly to the input dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make at least three non-trivial observations/discoveries about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df_new = pd.read_csv('data/995,000_rows.csv')\n",
    "# df_new = pd.read_csv('data/news_sample.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations of domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.process_a as process_a\n",
    "# print(\"Basic Statistics of the Dataset:\")\n",
    "# print(df_new.info())\n",
    "\n",
    "# Filter the DataFrame for 'reliable' and 'fake' types\n",
    "reliable_domains = set(df_new[df_new['type'] == 'reliable']['domain'].unique())\n",
    "fake_domains = set(df_new[df_new['type'] == 'fake']['domain'].unique())\n",
    "\n",
    "# Domains in 'reliable' but not in 'fake'\n",
    "reliable_not_fake_domains = reliable_domains - fake_domains\n",
    "\n",
    "# Domains in 'fake' but not in 'reliable'\n",
    "fake_not_reliable_domains = fake_domains - reliable_domains\n",
    "\n",
    "# Find the intersection of unique domains\n",
    "common_domains = fake_domains.intersection(reliable_domains)\n",
    "\n",
    "print(\"Domains in 'reliable' but not in 'fake':\")\n",
    "print(reliable_not_fake_domains)\n",
    "\n",
    "print(\"\\nDomains in 'fake' but not in 'reliable':\")\n",
    "print(fake_not_reliable_domains)\n",
    "\n",
    "print(\"\\nDomains in both 'fake' and 'reliable':\")\n",
    "print(common_domains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.process_methods as pm\n",
    "import swifter\n",
    "\n",
    "# Drop rows where either 'type' or 'content' is NaN\n",
    "df_new.dropna(subset=['type', 'content'], inplace=True)\n",
    "\n",
    "# Apply clean_text function\n",
    "df_new['content_clean'] = df_new['content'].swifter.apply(pm.clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to count URLs with <URL> tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to count URLs with <URL> tag\n",
    "def count_urls_with_tag(text):\n",
    "    urls_with_tag = re.findall('_url_', text)\n",
    "    return len(urls_with_tag)\n",
    "\n",
    "# Apply count URLs with <URL> tag\n",
    "df_new['url_count'] = df_new['content_clean'].apply(count_urls_with_tag)\n",
    "\n",
    "# print(df[['content_clean', 'url_count_with_tag']])\n",
    "\n",
    "# Total URLs in Content\n",
    "total_urls = df_new['url_count'].sum()\n",
    "print(\"Total URLs in Content:\", total_urls)\n",
    "\n",
    "# Filter DataFrame for articles labeled as 'fake'\n",
    "fake_articles = df_new[df_new['type'] == 'fake']\n",
    "\n",
    "# add more type\n",
    "#fake_articles = df_new[(df_new['type'] == 'fake') | (df_new['type'] == '')]\n",
    "\n",
    "# Total URLs in 'fake' content\n",
    "total_fake_urls = fake_articles['url_count'].sum()\n",
    "print(\"Total URLs in 'fake' content:\", total_fake_urls)\n",
    "\n",
    "# Filter DataFrame for articles labeled as 'reliable'\n",
    "reliable_articles = df_new[df_new['type'] == 'reliable']\n",
    "\n",
    "# Total URLs in 'reliable' content\n",
    "total_reliable_urls = reliable_articles['url_count'].sum()\n",
    "print(\"Total URLs in 'reliable' content:\", total_reliable_urls)\n",
    "\n",
    "# Minimum number of URLs in 'fake' content\n",
    "min_fake_urls = fake_articles['url_count'].min()\n",
    "print(\"Minimum number of URLs in 'fake' content:\", min_fake_urls)\n",
    "\n",
    "# Maximum number of URLs in 'fake' content\n",
    "max_fake_urls = fake_articles['url_count'].max()\n",
    "print(\"Maximum number of URLs in 'fake' content:\", max_fake_urls)\n",
    "\n",
    "# Mean number of URLs in 'fake' content\n",
    "mean_fake_urls = fake_articles['url_count'].mean()\n",
    "print(\"Mean number of URLs in 'fake' content:\", mean_fake_urls)\n",
    "\n",
    "# Minimum number of URLs in 'reliable' content\n",
    "min_reliable_urls = reliable_articles['url_count'].min()\n",
    "print(\"Minimum number of URLs in 'reliable' content:\", min_reliable_urls)\n",
    "\n",
    "# Maximum number of URLs in 'reliable' content\n",
    "max_reliable_urls = reliable_articles['url_count'].max()\n",
    "print(\"Maximum number of URLs in 'reliable' content:\", max_reliable_urls)\n",
    "\n",
    "# Mean number of URLs in 'reliable' content\n",
    "mean_reliable_urls = reliable_articles['url_count'].mean()\n",
    "print(\"Mean number of URLs in 'reliable' content:\", mean_reliable_urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to count DATEs with <DATE> tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count DATEs with <DATE> tag\n",
    "def count_date_with_tag(text):\n",
    "    date_with_tag = re.findall('_date_', text)\n",
    "    return len(date_with_tag)\n",
    "\n",
    "# Apply count DATEs with <DATE> tag\n",
    "df_new['date_count'] = df_new['content_clean'].apply(count_date_with_tag)\n",
    "\n",
    "# Total DATEs in Content\n",
    "total_dates = df_new['date_count'].sum()\n",
    "print(\"Total DATEs in Content:\", total_dates)\n",
    "\n",
    "# Filter DataFrame for articles labeled as 'fake'\n",
    "fake_articles = df_new[df_new['type'] == 'fake']\n",
    "\n",
    "# add more type\n",
    "#fake_articles = df_new[(df_new['type'] == 'fake') | (df_new['type'] == '')]\n",
    "\n",
    "# Total DATEs in 'fake' content\n",
    "total_fake_dates = fake_articles['date_count'].sum()\n",
    "print(\"Total DATEs in 'fake' content:\", total_fake_dates)\n",
    "\n",
    "# Filter DataFrame for articles labeled as 'reliable'\n",
    "reliable_articles = df_new[df_new['type'] == 'reliable']\n",
    "\n",
    "# Total URLs in 'reliable' content\n",
    "total_reliable_dates = reliable_articles['date_count'].sum()\n",
    "print(\"Total DATEs in 'reliable' content:\", total_reliable_dates)\n",
    "\n",
    "# Minimum number of DATEs in 'fake' content\n",
    "min_fake_dates = fake_articles['date_count'].min()\n",
    "print(\"Minimum number of DATEs in 'fake' content:\", min_fake_dates)\n",
    "\n",
    "# Maximum number of DATEs in 'fake' content\n",
    "max_fake_dates = fake_articles['date_count'].max()\n",
    "print(\"Maximum number of DATEs in 'fake' content:\", max_fake_dates)\n",
    "\n",
    "# Mean number of URLs in 'fake' content\n",
    "mean_fake_dates = fake_articles['date_count'].mean()\n",
    "print(\"Mean number of DATEs in 'fake' content:\", mean_fake_dates)\n",
    "\n",
    "# Minimum number of URLs in 'reliable' content\n",
    "min_reliable_dates = reliable_articles['date_count'].min()\n",
    "print(\"Minimum number of DATEs in 'reliable' content:\", min_reliable_dates)\n",
    "\n",
    "# Maximum number of URLs in 'reliable' content\n",
    "max_reliable_dates = reliable_articles['date_count'].max()\n",
    "print(\"Maximum number of DATEs in 'reliable' content:\", max_reliable_dates)\n",
    "\n",
    "# Mean number of URLs in 'reliable' content\n",
    "mean_reliable_dates = reliable_articles['date_count'].mean()\n",
    "print(\"Mean number of DATEs in 'reliable' content:\", mean_reliable_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to count NUMs with <NUM> tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to count NUMs with <NUM> tag\n",
    "def count_num_with_tag(text):\n",
    "    num_with_tag = re.findall('_num_', text)\n",
    "    return len(num_with_tag)\n",
    "\n",
    "# Apply count NUms with <NUM> tag\n",
    "df_new['num_count'] = df_new['content_clean'].apply(count_num_with_tag)\n",
    "\n",
    "# Total NUMs in Content\n",
    "total_nums = df_new['num_count'].sum()\n",
    "print(\"Total NUMs in Content:\", total_nums)\n",
    "\n",
    "# Filter DataFrame for articles labeled as 'fake'\n",
    "fake_articles = df_new[df_new['type'] == 'fake']\n",
    "\n",
    "# add more type\n",
    "#fake_articles = df_new[(df_new['type'] == 'fake') | (df_new['type'] == '')]\n",
    "\n",
    "# Total NUMs in 'fake' content\n",
    "total_fake_nums = fake_articles['num_count'].sum()\n",
    "print(\"Total NUMs in 'fake' content:\", total_fake_nums)\n",
    "\n",
    "# Filter DataFrame for articles labeled as 'reliable'\n",
    "reliable_articles = df_new[df_new['type'] == 'reliable']\n",
    "\n",
    "# Total NUMs in 'reliable' content\n",
    "total_reliable_nums = reliable_articles['num_count'].sum()\n",
    "print(\"Total NUMs in 'reliable' content:\", total_reliable_nums)\n",
    "\n",
    "# Minimum number of NUMs in 'fake' content\n",
    "min_fake_nums = fake_articles['num_count'].min()\n",
    "print(\"Minimum number of NUMs in 'fake' content:\", min_fake_nums)\n",
    "\n",
    "# Maximum number of NUMs in 'fake' content\n",
    "max_fake_nums = fake_articles['num_count'].max()\n",
    "print(\"Maximum number of NUMs in 'fake' content:\", max_fake_nums)\n",
    "\n",
    "# Mean number of NUMs in 'fake' content\n",
    "mean_fake_nums = fake_articles['num_count'].mean()\n",
    "print(\"Mean number of NUMs in 'fake' content:\", mean_fake_nums)\n",
    "\n",
    "# Minimum number of URLs in 'reliable' content\n",
    "min_reliable_nums = reliable_articles['num_count'].min()\n",
    "print(\"Minimum number of NUMs in 'reliable' content:\", min_reliable_nums)\n",
    "\n",
    "# Maximum number of URLs in 'reliable' content\n",
    "max_reliable_nums = reliable_articles['num_count'].max()\n",
    "print(\"Maximum number of NUMs in 'reliable' content:\", max_reliable_nums)\n",
    "\n",
    "# Mean number of URLs in 'reliable' content\n",
    "mean_reliable_nums = reliable_articles['num_count'].mean()\n",
    "print(\"Mean number of NUMs in 'reliable' content:\", mean_reliable_nums)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barplot for top 10000 most frequent clean words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize an empty counter to store word frequencies\n",
    "clean_word_freq = Counter()\n",
    "\n",
    "# Iterate over each row of the DataFrame\n",
    "for _, row in df_new.iterrows():\n",
    "    # Join the clean words in the 'content_clean' column of the current row into a single string\n",
    "    clean_text = ' '.join(re.findall(r'\\b\\w+\\b', row['content_clean']))\n",
    "    \n",
    "    # Count the word frequencies for the current row\n",
    "    clean_word_freq.update(clean_text.split())\n",
    "\n",
    "# Sort the word frequencies in descending order\n",
    "sorted_clean_word_freq = sorted(clean_word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Extract the 100 most frequent words\n",
    "top_100_clean_words = sorted_clean_word_freq[:100]\n",
    "\n",
    "# Print the top 100 most frequent words\n",
    "print(top_100_clean_words)\n",
    "\n",
    "# Extract the 10000 most frequent words\n",
    "top_10000_clean_words = sorted_clean_word_freq[:10000]\n",
    "\n",
    "# Barplot for top 10000 most frequent clean words\n",
    "plt.figure(figsize=(15, 6))\n",
    "words, frequencies = zip(*top_10000_clean_words)\n",
    "plt.bar(words, frequencies)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10000 Most Frequent Clean Words')\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.process_methods as pm\n",
    "import swifter\n",
    "# Apply remove_stopwords to 'content_clean' column and create 'content_stopword' column\n",
    "df_new['content_stopword'] = df_new['content_clean'].swifter.apply(pm.remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.process_methods as pm\n",
    "import swifter\n",
    "df_new['content_stem'] = df_new['content_stopword'].swifter.apply(pm.remove_word_variations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv('data/995,000_rows_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values\n",
    "print(\"\\nMissing Values in Each Column:\")\n",
    "print(df_new.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barplot for top 10000 most frequent clean words after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize an empty counter to store word frequencies\n",
    "clean_word_freq_after = Counter()\n",
    "\n",
    "# Iterate over each row of the DataFrame\n",
    "for _, row in df_new.iterrows():\n",
    "    # Join the clean words in the 'content_clean' column of the current row into a single string\n",
    "    clean_text_after = ' '.join(re.findall(r'\\b\\w+\\b', row['content_stem']))\n",
    "    \n",
    "    # Count the word frequencies for the current row\n",
    "    clean_word_freq_after.update(clean_text_after.split())\n",
    "\n",
    "# Sort in descending order\n",
    "sorted_clean_word_freq_after = sorted(clean_word_freq_after.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Extract the 100 most frequent words\n",
    "top_100_clean_words_after = sorted_clean_word_freq_after[:100]\n",
    "\n",
    "for word, frequency in top_100_clean_words_after:\n",
    "    print(f\"{word}: {frequency}\")\n",
    "\n",
    "# Extract the 10000 most frequent words\n",
    "top_10000_clean_words_after = sorted_clean_word_freq_after[:10000]\n",
    "\n",
    "# Barplot for top 10000 most frequent clean words\n",
    "plt.figure(figsize=(15, 6))\n",
    "words, frequencies = zip(*top_10000_clean_words_after)\n",
    "plt.bar(words, frequencies)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10000 Most Frequent Clean Words After Preprocessing')\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Apply preprocess to '995,000_rows.csv' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lib.process_b as process_b\n",
    "\n",
    "src = 'data/995,000_rows.csv'\n",
    "# src = 'data/995,000_rows_SAMPLE.csv'\n",
    "# src = 'data/news_sample.csv'\n",
    "dst = src[0:-4] + '_cleaned.csv'\n",
    "\n",
    "# preprocess\n",
    "df = pd.read_csv(src)\n",
    "process_b.preprocess(df)\n",
    "\n",
    "# save csv file copy of preprocessed dataframe\n",
    "df.to_csv(dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Split the dataset into a training, validation, and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lib.split as split\n",
    "\n",
    "src = 'data/995,000_rows.csv'\n",
    "# src = 'data/995,000_rows_SAMPLE.csv'\n",
    "# src = 'data/news_sample.csv'\n",
    "\n",
    "df = pd.read_csv(src)\n",
    "split.eighty_ten_ten(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
