{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a new dataset only with feature columns to be used for the simple LogReg model\n",
    "\n",
    "# load grouped training data\n",
    "src = '../data/training_data_grouped.csv'\n",
    "training_data = pd.read_csv(src)\n",
    "\n",
    "# load grouped validation data\n",
    "src = '../data/validation_data_grouped.csv'\n",
    "validation_data = pd.read_csv(src)\n",
    "\n",
    "# load grouped test data\n",
    "src = '../data/test_data_grouped.csv'\n",
    "test_data = pd.read_csv(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'Unnamed' columns\n",
    "training_data.drop(training_data.columns[[0, 1, 2]], axis=1, inplace=True)\n",
    "validation_data.drop(validation_data.columns[[0, 1, 2]], axis=1, inplace=True)\n",
    "test_data.drop(test_data.columns[[0, 1, 2]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make grouped types ('fake' or 'reliable') into 'true' or 'false' values\n",
    "def bool_dummies(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    type_data = pd.get_dummies(df['type'], drop_first=True)\n",
    "    df = pd.concat([df, type_data], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Bool value of 'reliable' to be used on y-axis when training model.\n",
    "# training data\n",
    "training_data = bool_dummies(training_data, 'type')\n",
    "\n",
    "# validation data\n",
    "validation_data = bool_dummies(validation_data, 'type')\n",
    "\n",
    "# test data\n",
    "test_data = bool_dummies(test_data, 'type')\n",
    "\n",
    "# save to file\n",
    "# training_data.to_csv('data/training_data_features.csv')\n",
    "# validation_data.to_csv('data/validation_data_features.csv')\n",
    "# test_data.to_csv('data/test_data_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to count tags, e.g. NUMs with <NUM> tag\n",
    "def count_tag(text: str, tag: str) -> int:\n",
    "    num_with_tag = re.findall(tag, text)\n",
    "    return len(num_with_tag)\n",
    "\n",
    "# Apply count NUMs with <NUM> tag\n",
    "num_tag = '_num_'\n",
    "training_data['num_count'] = training_data['content_clean'].apply(count_tag, tag=num_tag)\n",
    "validation_data['num_count'] = validation_data['content_clean'].apply(count_tag, tag=num_tag)\n",
    "test_data['num_count'] = test_data['content_clean'].apply(count_tag, tag=num_tag)\n",
    "\n",
    "# Apply count DATEs with <DATE> tag\n",
    "date_tag = '_date_'\n",
    "training_data['date_count'] = training_data['content_clean'].apply(count_tag, tag=date_tag)\n",
    "validation_data['date_count'] = validation_data['content_clean'].apply(count_tag, tag=date_tag)\n",
    "test_data['date_count'] = test_data['content_clean'].apply(count_tag, tag=date_tag)\n",
    "\n",
    "# Apply count URLs with <URL> tag\n",
    "url_tag = '_url_'\n",
    "training_data['url_count'] = training_data['content_clean'].apply(count_tag, tag=url_tag)\n",
    "validation_data['url_count'] = validation_data['content_clean'].apply(count_tag, tag=url_tag)\n",
    "test_data['url_count'] = test_data['content_clean'].apply(count_tag, tag=url_tag)\n",
    "\n",
    "# save to file\n",
    "# training_data.to_csv('data/training_data_features.csv')\n",
    "# validation_data.to_csv('data/validation_data_features.csv')\n",
    "# test_data.to_csv('data/test_data_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count single char in string\n",
    "def count_char(text: str, char: str):\n",
    "    return text.count(',')\n",
    "\n",
    "# count of commas in each article\n",
    "comma = ','\n",
    "training_data['comma_count'] = training_data['content_clean'].apply(count_char, char=comma)\n",
    "validation_data['comma_count'] = validation_data['content_clean'].apply(count_char, char=comma)\n",
    "test_data['comma_count'] = test_data['content_clean'].apply(count_char, char=comma)\n",
    "\n",
    "# count of exlamation points in each article\n",
    "exclm = '!'\n",
    "training_data['exclm_count'] = training_data['content_clean'].apply(count_char, char=exclm)\n",
    "validation_data['exclm_count'] = validation_data['content_clean'].apply(count_char, char=exclm)\n",
    "test_data['exclm_count'] = test_data['content_clean'].apply(count_char, char=exclm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristian/miniconda3/envs/fake-news-proj/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Pandas Apply: 100%|██████████| 346868/346868 [06:15<00:00, 922.81it/s] \n",
      "Pandas Apply: 100%|██████████| 43573/43573 [00:48<00:00, 898.94it/s] \n",
      "Pandas Apply: 100%|██████████| 43479/43479 [00:48<00:00, 889.29it/s] \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import swifter\n",
    "\n",
    "# Count unique words in text (word frequency of content_clean)\n",
    "def get_word_freq(text: str) -> int:\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return len(set(tokens))\n",
    "\n",
    "# get word freq\n",
    "training_data['content_word_freq'] = training_data['content_clean'].swifter.apply(get_word_freq)\n",
    "validation_data['content_word_freq'] = validation_data['content_clean'].swifter.apply(get_word_freq)\n",
    "test_data['content_word_freq'] = test_data['content_clean'].swifter.apply(get_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 346868/346868 [04:57<00:00, 1164.37it/s]\n",
      "Pandas Apply: 100%|██████████| 43573/43573 [00:37<00:00, 1162.24it/s]\n",
      "Pandas Apply: 100%|██████████| 43479/43479 [00:37<00:00, 1155.88it/s]\n",
      "Pandas Apply: 100%|██████████| 346868/346868 [04:37<00:00, 1252.09it/s]\n",
      "Pandas Apply: 100%|██████████| 43573/43573 [00:35<00:00, 1239.04it/s]\n",
      "Pandas Apply: 100%|██████████| 43479/43479 [00:35<00:00, 1227.44it/s]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import swifter\n",
    "\n",
    "# Count unique words in text (word frequency of content_clean)\n",
    "def get_word_freq(text: str) -> int:\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return len(set(tokens))\n",
    "\n",
    "# word freq after stopword removal\n",
    "training_data['stop_word_freq'] = training_data['content_stopword'].swifter.apply(get_word_freq)\n",
    "validation_data['stop_word_freq'] = validation_data['content_stopword'].swifter.apply(get_word_freq)\n",
    "test_data['stop_word_freq'] = test_data['content_stopword'].swifter.apply(get_word_freq)\n",
    "\n",
    "# word freq after stemming\n",
    "training_data['stem_word_freq'] = training_data['content_stem'].swifter.apply(get_word_freq)\n",
    "validation_data['stem_word_freq'] = validation_data['content_stem'].swifter.apply(get_word_freq)\n",
    "test_data['stem_word_freq'] = test_data['content_stem'].swifter.apply(get_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "training_data.to_csv('../data/training_data_features.csv')\n",
    "validation_data.to_csv('../data/validation_data_features.csv')\n",
    "test_data.to_csv('../data/test_data_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduction rate on stopword removal\n",
    "# training\n",
    "col_a = training_data['content_word_freq']\n",
    "col_b = training_data['stop_word_freq']\n",
    "training_data['stop_reduction_rate'] = round(((col_a - col_b)/col_a) * 100, 3)\n",
    "\n",
    "# validation\n",
    "col_a = validation_data['content_word_freq']\n",
    "col_b = validation_data['stop_word_freq']\n",
    "validation_data['stop_reduction_rate'] = round(((col_a - col_b)/col_a) * 100, 3)\n",
    "\n",
    "# test\n",
    "col_a = test_data['content_word_freq']\n",
    "col_b = test_data['stop_word_freq']\n",
    "test_data['stop_reduction_rate'] = round(((col_a - col_b)/col_a) * 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduction rate on stem removal\n",
    "# training\n",
    "col_a = training_data['content_word_freq']\n",
    "col_b = training_data['stem_word_freq']\n",
    "training_data['stem_reduction_rate'] = round(((col_a - col_b)/col_a) * 100, 3)\n",
    "\n",
    "# validation\n",
    "col_a = validation_data['content_word_freq']\n",
    "col_b = validation_data['stem_word_freq']\n",
    "validation_data['stem_reduction_rate'] = round(((col_a - col_b)/col_a) * 100, 3)\n",
    "\n",
    "# test\n",
    "col_a = test_data['content_word_freq']\n",
    "col_b = test_data['stem_word_freq']\n",
    "test_data['stem_reduction_rate'] = round(((col_a - col_b)/col_a) * 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "training_data.to_csv('../data/training_data_features.csv')\n",
    "validation_data.to_csv('../data/validation_data_features.csv')\n",
    "test_data.to_csv('../data/test_data_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 346868/346868 [00:06<00:00, 50249.39it/s]\n",
      "Pandas Apply: 100%|██████████| 43573/43573 [00:00<00:00, 49500.48it/s]\n",
      "Pandas Apply: 100%|██████████| 43479/43479 [00:00<00:00, 50528.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# (Avarage of use of words per sentence. per article)\n",
    "\n",
    "import swifter\n",
    "\n",
    "def average_sentence_length(text):\n",
    "    # Split the text into sentences\n",
    "    sentences = text.split('.')\n",
    "    \n",
    "    # Initialize variables to store total length and number of sentences\n",
    "    total_length = 0\n",
    "    num_sentences = 0\n",
    "    \n",
    "    # Iterate through each sentence to calculate total length and count the number of sentences\n",
    "    for sentence in sentences:\n",
    "        # Count the number of words in the sentence\n",
    "        words = sentence.split()\n",
    "        length = len(words)\n",
    "        \n",
    "        # Add the length of the current sentence to the total length\n",
    "        total_length += length\n",
    "        \n",
    "        # Increment the number of sentences\n",
    "        if length > 0:  # Exclude empty sentences\n",
    "            num_sentences += 1\n",
    "    \n",
    "    # Calculate the average length of sentences\n",
    "    if num_sentences > 0:\n",
    "        average_length = total_length / num_sentences\n",
    "    else:\n",
    "        average_length = 0\n",
    "    \n",
    "    return int(average_length)\n",
    "\n",
    "# Apply\n",
    "training_data['average_sentence_length'] = training_data['content'].swifter.apply(average_sentence_length)\n",
    "validation_data['average_sentence_length'] = validation_data['content'].swifter.apply(average_sentence_length)\n",
    "test_data['average_sentence_length'] = test_data['content'].swifter.apply(average_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True or false value for authors* [meta feature]\n",
    "training_data['has_author'] = training_data['authors'].notnull()\n",
    "validation_data['has_author'] = validation_data['authors'].notnull()\n",
    "test_data['has_author'] = test_data['authors'].notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "training_data.to_csv('../data/training_data_features.csv')\n",
    "validation_data.to_csv('../data/validation_data_features.csv')\n",
    "test_data.to_csv('../data/test_data_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
