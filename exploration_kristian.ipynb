{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load raw dataset\n",
    "src = 'data/995,000_rows.csv'\n",
    "# src = 'data/SAMPLE.csv'\n",
    "raw_data = pd.read_csv(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amount of '!' (exclamations) in fake news vs. reliable news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a lot more '!'-characters in fake labelled articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "char = '!'\n",
    "\n",
    "# copy dataframe\n",
    "exclm_data = raw_data.copy(deep=True)\n",
    "\n",
    "# get count sum of exclamation points in each article\n",
    "exclm_data['exclm_count'] = exclm_data['content'].str.count(char)\n",
    "\n",
    "# get total sum of exclamation points for each type (labels)\n",
    "fake_exclm_sum = (exclm_data[ (exclm_data['type'] == 'fake')])['exclm_count'].mean()\n",
    "reliable_exclm_sum = (exclm_data[ (exclm_data['type'] == 'reliable')])['exclm_count'].mean()\n",
    "\n",
    "# plot data\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_ylabel('mean')\n",
    "ax.set_title('\\'!\\' characters in fake vs. reliable')\n",
    "\n",
    "ax.bar(['fake', 'reliable'], [fake_exclm_sum, reliable_exclm_sum])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amount of '!' (exclamations) in each type of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'political' has most exclamations points. Second is 'fake'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "char = '!'\n",
    "\n",
    "# copy dataframe\n",
    "exclm_data = raw_data.copy(deep=True)\n",
    "\n",
    "# get count sum of exclamation points in each article\n",
    "exclm_data['exclm_count'] = exclm_data['content'].str.count(char)\n",
    "\n",
    "# get total sum of exclamation points for each type (labels)\n",
    "types = ['reliable',\n",
    "         'political',\n",
    "         'bias',\n",
    "         'fake',\n",
    "         'conspiracy',\n",
    "         'rumor',\n",
    "         'unknown',\n",
    "         'unreliable',\n",
    "         'clickbait',\n",
    "         'junksci',\n",
    "         'satire',\n",
    "         'hate'\n",
    "         ]\n",
    "\n",
    "sums = []\n",
    "for type in types:\n",
    "    sum = (exclm_data[ (exclm_data['type'] == type)])['exclm_count'].mean()\n",
    "    sums.append(sum)\n",
    "\n",
    "# plot data\n",
    "fig, ax = plt.subplots()\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "ax.set_ylabel('mean')\n",
    "ax.set_title('\\'!\\' characters in all article types')\n",
    "\n",
    "ax.bar(types, sums)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amount of unique words in reliable news vs. fake news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the mean of different words for each article type. The results shows that 'reliable' has more unique words then 'fake'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path\n",
    "import nltk\n",
    "import swifter\n",
    "\n",
    "# This method has to be used on the cleaned dataset, otherwise \n",
    "# we'll get errors if running tokenizing of words on the unprocessed content text.\n",
    "\n",
    "src = 'data/995,000_rows_cleaned.csv'\n",
    "# src = 'data/SAMPLE_cleaned.csv'\n",
    "dst = 'data/word_freq.csv'\n",
    "\n",
    "def get_word_freq(text: str) -> int:\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return len(set(tokens))\n",
    "\n",
    "if not os.path.isfile(dst):\n",
    "    # load data\n",
    "    word_data = pd.read_csv(src)\n",
    "    word_freq = pd.DataFrame()\n",
    "    # get word freq\n",
    "    word_freq['content_word_freq'] = word_data['content_clean'].swifter.apply(get_word_freq)\n",
    "    # concat types\n",
    "    word_freq.insert(0, \"type\", word_data['type'])\n",
    "    # save to file\n",
    "    word_freq.to_csv(dst)\n",
    "else:\n",
    "    print('File already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Data is loaded from a new file 'word_freq' to minimize the file-size on the cleaned dataset.\n",
    "# The filesize of 'word_freq' is small.\n",
    "\n",
    "src = 'data/word_freq.csv'\n",
    "word_freq = pd.read_csv(src)\n",
    "\n",
    "# get total mean of exclamation points for each type (labels)\n",
    "types = ['reliable',\n",
    "         'political',\n",
    "         'bias',\n",
    "         'fake',\n",
    "         'conspiracy',\n",
    "         'rumor',\n",
    "         'unknown',\n",
    "         'unreliable',\n",
    "         'clickbait',\n",
    "         'junksci',\n",
    "         'satire',\n",
    "         'hate'\n",
    "         ]\n",
    "\n",
    "means = []\n",
    "for type in types:\n",
    "    mean = (word_freq[ (word_freq['type'] == type)])['content_word_freq'].mean()\n",
    "    means.append(mean)\n",
    "\n",
    "# plot data\n",
    "fig, ax = plt.subplots()\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "ax.set_ylabel('mean')\n",
    "ax.set_title('Unique words in articles by type')\n",
    "\n",
    "ax.bar(types, means)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Fake news have less author names then reliable news? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the barplot, it seems that 'reliable' news have more missing authors, then 'fake' news. So actually the opposite of our hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get all rows with label 'fake'\n",
    "fake_data = raw_data[(raw_data['type'] == 'fake')]\n",
    "\n",
    "# count rows for 'fake' with no author names \n",
    "fake_auth_isNull_sum = fake_data['authors'].isnull().sum()\n",
    "\n",
    "# get all rows with label 'reliable'\n",
    "reliable_data = raw_data[(raw_data['type'] == 'reliable')]\n",
    "\n",
    "# count rows for 'reliable' with no author names\n",
    "reliable_auth_isNull_sum = reliable_data['authors'].isnull().sum()\n",
    "\n",
    "# plot comparison\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_ylabel('missing author values')\n",
    "ax.set_title('Missing author values: \\'fake\\' vs. \\'reliable\\' news')\n",
    "\n",
    "ax.bar(['fake', 'reliable'], [fake_auth_isNull_sum, reliable_auth_isNull_sum])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations between word reduction rates between reliable vs. fake?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
