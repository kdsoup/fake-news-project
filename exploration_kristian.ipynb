{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load raw dataset\n",
    "src = 'data/995,000_rows.csv'\n",
    "# src = 'data/SAMPLE.csv'\n",
    "raw_data = pd.read_csv(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amount of '!' (exclamations) in fake news vs. reliable news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a lot more '!'-characters in fake labelled articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "char = '!'\n",
    "\n",
    "# copy dataframe\n",
    "exclm_data = raw_data.copy(deep=True)\n",
    "\n",
    "# get count sum of exclamation points in each article\n",
    "exclm_data['exclm_count'] = exclm_data['content'].str.count(char)\n",
    "\n",
    "# get total sum of exclamation points for each type (labels)\n",
    "fake_exclm_sum = (exclm_data[ (exclm_data['type'] == 'fake')])['exclm_count'].mean()\n",
    "reliable_exclm_sum = (exclm_data[ (exclm_data['type'] == 'reliable')])['exclm_count'].mean()\n",
    "\n",
    "# plot data\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_ylabel('mean')\n",
    "ax.set_title('\\'!\\' characters in fake vs. reliable')\n",
    "\n",
    "ax.bar(['fake', 'reliable'], [fake_exclm_sum, reliable_exclm_sum])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amount of '!' (exclamations) in each type of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'political' has most exclamations points. Second is 'fake'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "char = '!'\n",
    "\n",
    "# copy dataframe\n",
    "exclm_data = raw_data.copy(deep=True)\n",
    "\n",
    "# get count sum of exclamation points in each article\n",
    "exclm_data['exclm_count'] = exclm_data['content'].str.count(char)\n",
    "\n",
    "# get total sum of exclamation points for each type (labels)\n",
    "types = ['reliable',\n",
    "         'political',\n",
    "         'bias',\n",
    "         'fake',\n",
    "         'conspiracy',\n",
    "         'rumor',\n",
    "         'unknown',\n",
    "         'unreliable',\n",
    "         'clickbait',\n",
    "         'junksci',\n",
    "         'satire',\n",
    "         'hate'\n",
    "         ]\n",
    "\n",
    "sums = []\n",
    "for type in types:\n",
    "    sum = (exclm_data[ (exclm_data['type'] == type)])['exclm_count'].mean()\n",
    "    sums.append(sum)\n",
    "\n",
    "# plot data\n",
    "fig, ax = plt.subplots()\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "ax.set_ylabel('mean')\n",
    "ax.set_title('\\'!\\' characters in all article types')\n",
    "\n",
    "ax.bar(types, sums)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amount of unique words in reliable news vs. fake news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the mean of different words for each article type. The results shows that 'reliable' has more unique words then 'fake'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Data is loaded from a new file 'word_freq' to minimize the file-size on the cleaned dataset.\n",
    "# The filesize of 'word_freq' is small.\n",
    "\n",
    "src = 'data/word_freq.csv'\n",
    "word_freq = pd.read_csv(src)\n",
    "\n",
    "# get total mean of exclamation points for each type (labels)\n",
    "types = ['reliable',\n",
    "         'political',\n",
    "         'bias',\n",
    "         'fake',\n",
    "         'conspiracy',\n",
    "         'rumor',\n",
    "         'unknown',\n",
    "         'unreliable',\n",
    "         'clickbait',\n",
    "         'junksci',\n",
    "         'satire',\n",
    "         'hate'\n",
    "         ]\n",
    "\n",
    "means = []\n",
    "for type in types:\n",
    "    mean = (word_freq[ (word_freq['type'] == type)])['content_word_freq'].mean()\n",
    "    means.append(mean)\n",
    "\n",
    "# plot data\n",
    "fig, ax = plt.subplots()\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "ax.set_ylabel('mean')\n",
    "ax.set_title('Unique words in articles by type')\n",
    "\n",
    "ax.bar(types, means)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Fake news have less author names then reliable news? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the barplot, it seems that 'reliable' news have more missing authors, then 'fake' news. So actually the opposite of our hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get all rows with label 'fake'\n",
    "fake_data = raw_data[(raw_data['type'] == 'fake')]\n",
    "\n",
    "# count rows for 'fake' with no author names \n",
    "fake_auth_isNull_sum = fake_data['authors'].isnull().sum()\n",
    "\n",
    "# get all rows with label 'reliable'\n",
    "reliable_data = raw_data[(raw_data['type'] == 'reliable')]\n",
    "\n",
    "# count rows for 'reliable' with no author names\n",
    "reliable_auth_isNull_sum = reliable_data['authors'].isnull().sum()\n",
    "\n",
    "# plot comparison\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_ylabel('missing author values')\n",
    "ax.set_title('Missing author values: \\'fake\\' vs. \\'reliable\\' news')\n",
    "\n",
    "ax.bar(['fake', 'reliable'], [fake_auth_isNull_sum, reliable_auth_isNull_sum])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations between word reduction rates between reliable vs. fake?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring if 'fake' have an avarage higher reduction rate then 'reliable'. The results shows the 'fake' has a higher reduction rate then 'reliable'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data is loaded from a new file 'word_freq' to minimize the file-size on the cleaned dataset.\n",
    "# The filesize of 'word_freq' is small.\n",
    "\n",
    "src = 'data/word_freq.csv'\n",
    "word_freq = pd.read_csv(src)\n",
    "\n",
    "# get total mean of exclamation points for each type (labels)\n",
    "types = ['reliable',\n",
    "         'political',\n",
    "         'bias',\n",
    "         'fake',\n",
    "         'conspiracy',\n",
    "         'rumor',\n",
    "         'unknown',\n",
    "         'unreliable',\n",
    "         'clickbait',\n",
    "         'junksci',\n",
    "         'satire',\n",
    "         'hate'\n",
    "         ]\n",
    "\n",
    "stop_means = []\n",
    "stem_means = []\n",
    "for type in types:\n",
    "    stop_mean = (word_freq[ (word_freq['type'] == type)])['stop_reduction_rate'].mean()\n",
    "    stem_mean = (word_freq[ (word_freq['type'] == type)])['stem_reduction_rate'].mean()\n",
    "    stop_means.append(round(stop_mean, 1))\n",
    "    stem_means.append(round(stem_mean, 1))\n",
    "\n",
    "# plot data\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "x = np.arange(len(types))\n",
    "width = 0.4  # the width of the bars\n",
    "offset = width/2\n",
    "\n",
    "stop_bar = ax.bar(x-offset, stop_means, width, color='cyan')\n",
    "ax.bar_label(stop_bar, stop_means, padding=-25, rotation=90)\n",
    "\n",
    "stem_bar = ax.bar(x+offset, stem_means, width, color='orange')\n",
    "ax.bar_label(stem_bar, stem_means, padding=-25, rotation=90)\n",
    "\n",
    "ax.set_ylabel('Mean')\n",
    "ax.set_title('Stopword reduction rate')\n",
    "plt.xticks(x, types, rotation='vertical')\n",
    "plt.legend([\"Stopword\", \"Stemming\"])\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
